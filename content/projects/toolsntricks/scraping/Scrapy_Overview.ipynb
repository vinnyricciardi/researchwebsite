{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy overview<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this post is to show you how to use [Scrapy](https://scrapy.org/), a web scraping framework written in Python. Scrapy works by setting up a 'spider' to 'crawl' through web... pages (gotta love a solid program with a punny setup ;) ). We need to specify how we want our spider to crawl through the webpages (what links to follow) and what to pick up along its way (the data that each webpage stores in its html tags).\n",
    "\n",
    "To demonstrate how to set up a basic scraper, we will scrape all the articles' publication date, headline title, brief description, and the url link to the actual article from [EKurd](http://ekurd.net/all-news), a site dedicated to independent news on Kurdistan. This task is part of a larger project with friends at the awesome think tank [Aleph Policy Initiative](http://alephpolicy.org/) whose goal is to bridge data science and policy in the Middle East.\n",
    "\n",
    "We're going to use Scrapy for this task, because its a simple to use and to implement scraping framework that's ripe for iterating through those 'next' page features in EKurd's news archive.\n",
    "\n",
    "To make this work we use Python 3.5.2, Scrapy 1.4.0, and Pandas 0.21.0.\n",
    "\n",
    "The basic steps to scrape this site are we're going to grab the html elements out of the source code using the html tags (such as `div` tags, `h2` tags, `p` tags, and a few more). Then we'll store it in a Pandas dataframe, iterate to the next article in the list, store it and so on until we reach the end of the page. Then we'll have our spider crawl to the next page and repeat. Once our spider hits the last page, it will output the Pandas dataframe into an easy to analyze csv.\n",
    "\n",
    "To begin, we'll take an in depth look at EKurd's news webpage. It contains a list of articles with their publication date, headline title, brief description, and the url link to the actual article. At the bottom of the webpage there are one of those next page navigation buttons (see pic below) that will let you navigate to all 1300+ pages with the same type of information.\n",
    "\n",
    "<img src=\"{filename}/images/next_nav.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "That's a lot of news! With Scrapy, we automate all this so we can use this data in more interesting ways such as sentiment analysis and other types of text classification (stay tuned for a later post)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## 1. Take a look at EKurd's source code.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to where Ekurd keeps their [news archive](http://ekurd.net/all-news), right click on the first headline you see and click 'Inspect Element' if using Firefox - most browsers have a similar feature. See the screenshot below for reference.<br><br>\n",
    "\n",
    "<img src=\"{filename}/images/dev_console.png\" alt=\"Drawing\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look in the development console you can see that each articles' content (e.g., title, description, date published, and url) are listed under the div tag with the id set as 'Content.' Here's a screen shot of all the articles on this page neatly organized under the content `div` tag. We can see the first article is the one we right clicked on (note, when you right click on the article and inspect element, it will be expanded - I just wanted to show you the html hierarchy its embedded in).\n",
    "<br>\n",
    "\n",
    "<img src=\"{filename}/images/div_tag.png\" alt=\"Drawing\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to look at the first article in the list to see how it's structured. When you right clicked on the article title and opened the development console it should have expanded the article tag for the first article to look like this.\n",
    "\n",
    "<img src=\"{filename}/images/article_tag.png\" alt=\"Drawing\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the very first line under the `article` tag, the class is an `entry-list`. This information will come in handy when we set up are scraper because all the data we want to grab is under this `article` tag. If we keep reading through the code we can see that under the `h2` tag there is an `a` tag that contains the article's title and url to the actual article (note, the url is listed in many tags so we could grab it from a number of places). We can see in the `p` tag there is the description of the article and in the `aside` tag there is the publication date. \n",
    "\n",
    "To make use of these tags and the data they contain, we will need to tell Scrapy this is where we want our spider to grab data from.\n",
    "\n",
    "For now, just take note of the location of this data and I'll explain how they'll fit into our code below more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## 2. Writing a spider<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a boilerplate script for our spider. We'll first define the variables needed then I'll explain how it all is put together.\n",
    "\n",
    "We have to tell it what website or list of websites we want to scrape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_urls = ['http://ekurd.net/all-news']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And where to start looking for the data we want to grab. So we use the tags we identified in the source code before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TITLE_SELECTOR = 'h2 a ::text'        # This is where the title is stored                         \n",
    "URL_SELECTOR = 'h2 a ::attr(href)'    # This is where the url to the actual article is stored       \n",
    "DESCRIP_SELECTOR = 'p ::text'         # This is where the article description is stored           \n",
    "DATE_SELECTOR = 'aside a time::text'  # This is where the publication date is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these tags are used throughout the document in many ways, we need to use the article tag class to make sure we're in the right section of the source code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SET_SELECTOR = '.entry-list'          # This is the article class all the other tags fall under"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way this works is the spider will look for the first tag with the class of 'entry-list', then for each variable it will look for the subsequent tag. For example, take the `TITLE_SELECTOR` variable. The spider will look under the first tag with an `entry-list` then for the `h2` tag, then for the `a` tag, then grab all the text. The tags are to the left of the double colons, what you want to grab is to the right. This is the beauty of Scrapy, it parses the html file for you so you don't have to make long regex or other lookups.\n",
    "\n",
    "Then it's as simple as making a loop to loop through all the `article` tags identified as an entry-list.\n",
    "\n",
    "Here's the boilerplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class BrickSetSpider(scrapy.Spider):\n",
    "    \n",
    "    name = 'ekurd_spider'\n",
    "    start_urls = ['http://ekurd.net/all-news']\n",
    "        \n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        This is our main html parser.\n",
    "        \"\"\"\n",
    "        \n",
    "        # We set the base tag we're going to search through for our data\n",
    "        SET_SELECTOR = '.entry-list'\n",
    "        \n",
    "        # Then iterate through the base tag to get the data we need\n",
    "        for news in response.css(SET_SELECTOR):\n",
    "            \n",
    "            # We define each variable according to its heirarchy\n",
    "            # Ex: The DATE_SELECTOR variable would read: \n",
    "            #     under the aside tag look for the a tag then look for the time tag and grab the text\n",
    "            DATE_SELECTOR = 'aside a time::text'  \n",
    "            TITLE_SELECTOR = 'h2 a ::text'\n",
    "            DESCRIP_SELECTOR = 'p ::text'\n",
    "            \n",
    "            # We can also grab other non-text elements within tags, like href (urls)\n",
    "            URL_SELECTOR = 'h2 a ::attr(href)' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a scrapy spider type:\n",
    "\n",
    "`scrapy runspider spider.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now this script just goes crawls over the webpage we provided, saves and rewrites variables. But we need it to save the data in a structured way.\n",
    "\n",
    "An easy way to save this data is to capture each variable into a list, then toss it into a Pandas dataframe.\n",
    "We can do this by defining an empty dataframe when we initialize the spider along with several empty lists (one per variable) that we'll use to fill in the Pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BrickSetSpider(scrapy.Spider):\n",
    "\n",
    "    name = 'ekurd_spider'\n",
    "    start_urls = ['http://ekurd.net/all-news']\n",
    "\n",
    "    # Here we can initialize the script\n",
    "    def __init__(self):\n",
    "    \n",
    "        # Create an empty Pandas dataframe, and an empty list per variable\n",
    "        self.out = pd.DataFrame(columns=['date', 'title', 'description', 'url'])\n",
    "        self.dates = []\n",
    "        self.titles = []\n",
    "        self.descrips = []\n",
    "        self.urls = []\n",
    "\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        SET_SELECTOR = '.entry-list'\n",
    "        \n",
    "        for news in response.css(SET_SELECTOR):\n",
    "\n",
    "            DATE_SELECTOR = 'aside a time::text'\n",
    "            TITLE_SELECTOR = 'h2 a ::text'\n",
    "            DESCRIP_SELECTOR = 'p ::text'\n",
    "            URL_SELECTOR = 'h2 a ::attr(href)'\n",
    "\n",
    "            # We extract the data from each variable and save it in the corresponding list\n",
    "            self.dates.append(news.css(DATE_SELECTOR).extract())\n",
    "            self.titles.append(news.css(TITLE_SELECTOR).extract())\n",
    "            self.descrips.append(news.css(DESCRIP_SELECTOR).extract_first())\n",
    "            self.urls.append(news.css(URL_SELECTOR).extract())\n",
    "        \n",
    "        # Once the lists are complete (the entire page has been scraped) we save it to\n",
    "        # a temporary dataframe (out2) that will be later concatenated with our main dataframe (out)\n",
    "        \n",
    "        # Create the temporary dataframe\n",
    "        out2 = pd.DataFrame(columns=['date', 'title', 'description', 'url'])\n",
    "        \n",
    "        # Save each list to the dataframe\n",
    "        out2['date'] = self.dates\n",
    "        out2['title'] = self.titles\n",
    "        out2['description'] = self.descrips\n",
    "        out2['url'] = self.urls\n",
    "        \n",
    "        # Ensure type is string (useful to bypass any non properly formatted date time and other possible errors)\n",
    "        for c in out2.columns:\n",
    "            out2[c] = out2[c].astype(str)\n",
    "        \n",
    "        # Merge it with our main dataframe (out)\n",
    "        self.out = pd.concat([self.out, out2], axis=0)\n",
    "        \n",
    "        # Save it to file\n",
    "        self.out.to_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a fully functioning spider! But it only scrapes the first page. We need it to iterate through all those next buttons to grab the entire database.\n",
    "\n",
    "Right click on the next button at the bottom of the page. For this site, the next button looks like two carets `>>`.\n",
    "\n",
    "<img src=\"{filename}/images/next_nav.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "We can see in the source code that the next button is defined in an a tag where the class equals `nextpostslink`. To use this info we can set a variable `NEXT_PAGE_SELECTOR` to grab the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEXT_PAGE_SELECTOR = \"//a[@class='nextpostslink']/@href\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the syntax for this variable is a bit different that what we used previously to grab a link because I wanted to demonstrate one more way to do this. This method uses the `xpath` syntax, which is super useful and often used in the forums so its good to be aware of. Basically, this can be read as, the `//` are the relative path in the html hierarchy to the a tag that has a class of `nextpostslink` that we want to grab the `href` (url) from. It comes in handy when you need to traverse html hierarchies.\n",
    "\n",
    "Once we have defined where the url to the next page lives in the source code, we can make a little conditional yield loop so that while there is a next page url on the webpage we want our spider to click it, go to the next page, and scrape the data (remember, we're going to have our spider crawl through many webpages and the very last page won't have a next button)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BrickSetSpider(scrapy.Spider):\n",
    "\n",
    "    name = 'ekurd_spider'\n",
    "    start_urls = ['http://ekurd.net/all-news']\n",
    "\n",
    "    def __init__(self):\n",
    "    \n",
    "        self.out = pd.DataFrame(columns=['date', 'title', 'description', 'url'])\n",
    "        self.dates = []\n",
    "        self.titles = []\n",
    "        self.descrips = []\n",
    "        self.urls = []\n",
    "\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        SET_SELECTOR = '.entry-list'\n",
    "        \n",
    "        # We define our link to the next page\n",
    "        NEXT_PAGE_SELECTOR = \"//a[@class='nextpostslink']/@href\"\n",
    "        # We extract the link using xpath\n",
    "        next_page = response.xpath(NEXT_PAGE_SELECTOR).extract_first()\n",
    "\n",
    "        for news in response.css(SET_SELECTOR):\n",
    "\n",
    "            DATE_SELECTOR = 'aside a time::text'\n",
    "            TITLE_SELECTOR = 'h2 a ::text'\n",
    "            DESCRIP_SELECTOR = 'p ::text'\n",
    "            URL_SELECTOR = 'h2 a ::attr(href)'\n",
    "\n",
    "            self.dates.append(news.css(DATE_SELECTOR).extract())\n",
    "            self.titles.append(news.css(TITLE_SELECTOR).extract())\n",
    "            self.descrips.append(news.css(DESCRIP_SELECTOR).extract_first())\n",
    "            self.urls.append(news.css(URL_SELECTOR).extract())\n",
    "        \n",
    "        # We put a conditional if there is a next page url then rerun the parser\n",
    "        if next_page:\n",
    "            yield scrapy.Request(\n",
    "                response.urljoin(next_page),\n",
    "                callback=self.parse\n",
    "            )\n",
    "\n",
    "        out2 = pd.DataFrame(columns=['date', 'title', 'description', 'url'])\n",
    "\n",
    "        out2['date'] = self.dates\n",
    "        out2['title'] = self.titles\n",
    "        out2['description'] = self.descrips\n",
    "        out2['url'] = self.urls\n",
    "\n",
    "        for c in out2.columns:\n",
    "            out2[c] = out2[c].astype(str)\n",
    "\n",
    "        self.out = pd.concat([self.out, out2], axis=0)\n",
    "        self.out.to_csv('data/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! This should crawl through EKurd and scrape the data we're looking for.\n",
    "\n",
    "To adapt this code for other similarly structured sites you only need to change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_urls = ['http://ekurd.net/all-news']\n",
    "SET_SELECTOR = '.entry-list'\n",
    "NEXT_PAGE_SELECTOR = \"//a[@class='nextpostslink']/@href\"\n",
    "DATE_SELECTOR = 'aside a time::text'\n",
    "TITLE_SELECTOR = 'h2 a ::text'\n",
    "DESCRIP_SELECTOR = 'p ::text'\n",
    "URL_SELECTOR = 'h2 a ::attr(href)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a much more in depth and awesome run down of Scrapy check out this post at [Digital Ocean.](https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3)\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_scrapy",
   "language": "python",
   "name": "py3_scrapy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
